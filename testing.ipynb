{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d9f32f-80b0-4bcf-8d37-534621eb16a1",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/93dtSTS.png\" style=\"height: 66px;\">\n",
    "\n",
    "# whisp/poc: `coralnu` coreference resolution\n",
    "---\n",
    "\n",
    "**task:** refactor inference code for both `neuralcoref` and `AllenNLP` to support GPU/CUDA-enabled inference so as to reduce processing times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d44bc-0ab6-4703-af80-5b6e5eb53a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## worklog from original implementation\n",
    "collapse me to get to the code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73398b-d841-4066-9313-8d1e96c94b69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### ~~Initial Server Setup~~\n",
    "> - Launched AWS EC2 `t3.xlarge` instance\n",
    "> - Connection via `coref.pem` file\n",
    "    - Stored in S3 at [s3://whisp-research-keys/ensemble-coref/](https://whisp-research-keys.s3.eu-west-2.amazonaws.com/ensemble-coref/coref.pem)\n",
    "> - `sudo apt-get update` and `sudo apt-get upgrade`\n",
    "> - Install Anaconda\n",
    "    - Make new env: `conda create --name coref python=3.6`\n",
    "    - Updates `conda update conda --all` and `conda update anaconda`\n",
    "    - Activate env `conda activate coref`\n",
    "        - Install GitHub CLI `conda install gh --channel conda-forge`\n",
    "            - Run `gh auth login` to authenticate and `gh auth setup-git`\n",
    "            \n",
    "### ~~Environment for Coref~~\n",
    "> - Clone GH repo [NeuroSYS-pl/coreference-resolution](https://github.com/NeuroSYS-pl/coreference-resolution)\n",
    "> - `conda activate coref`\n",
    "> - install `gcc` and `make` with `sudo apt-get install make gcc`\n",
    "    - also `sudo apt-get install python3-dev`\n",
    "\n",
    "**Installation instructions for dependencies (stolen from OG repo)**\n",
    "```\n",
    "pip install spacy==2.1\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install neuralcoref --no-binary neuralcoref\n",
    "pip install allennlp\n",
    "pip install --pre allennlp-models\n",
    "```\n",
    "\n",
    "@lucafrost — failing on install of spaCy due to C / Cython\n",
    "- retry after running `conda install -c conda-forge gcc` failed as `cc1plus` fails to execute\n",
    "    - as per [StackOverflow](https://stackoverflow.com/questions/69485181/how-to-install-g-on-conda-under-linux), retrying with `conda install -c conda-forge gxx` and `conda install -c conda-forge cxx-compiler`\n",
    "    \n",
    "~ 17/10/22\n",
    "\n",
    "---\n",
    "### Restarting efforts in Jupyter\n",
    "- The EC2 instance continues not to cooperate in building spaCy, specifically a package called `preshed` — this issue appears to be caused by the installation (or lack thereof) of a C++ compiler for some relevant cython code.\n",
    "    - I have tried installing every remedial solution I could find, including `gxx`, `cxx-compiler`, `python-dev`, etc...\n",
    "    - While lazy, a managed environment makes the most sense, AWS will not have data science clients encountering C++/ObjC errors.\n",
    "- Instantiated a `Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)` kernel image in AWS SageMaker Studio.\n",
    "- Clone git repository [NeuroSYS-pl/coreference-resolution](https://github.com/NeuroSYS-pl/coreference-resolution)\n",
    "- Successfully installed dependencies as below...\n",
    "```console\n",
    "pip install spacy==2.1\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install neuralcoref --no-binary neuralcoref\n",
    "pip install allennlp\n",
    "pip install --pre allennlp-models\n",
    "```\n",
    "*\\**I ran these in-notebook with `!pip` but I think an Image Terminal will also suffice*\n",
    "\n",
    "right, now to get to the actual work...\n",
    "\n",
    "~ 18/10/22 :: 13:44 GST\n",
    "\n",
    "---\n",
    "\n",
    "### updates\n",
    "- coref resolution with spaCy neuralcoref is up and running, ran into an issue with the `Predictor` class in AllenNLP: missing package 'ipywidgets'\n",
    "    - fix with `pip install ipywidgets` & restart kernel\n",
    "- ran into issue with kernel death upon calling `predictor = Predictor.from_path(model_url)`\n",
    "    - silly me, the instance only had 4GB of memory, upgrading to `ml.g4dn.xlarge`\n",
    "- all done with both spaCy neuralcoref and AllenNLP pretrained SpanBERT. have used the intersection strategies implemented by @mmaslankowska-neurosys.\n",
    "    - anecdotally, the `FuzzyIntersectionStrategy` appears to be the most effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cc470-4daf-4978-8549-08811f2675e3",
   "metadata": {},
   "source": [
    "## worklog for GPU refactor work\n",
    "-- 2022-12-13 @lucafrost\n",
    "\n",
    "**Environment Stuff**\n",
    "* Install dependencies, namely `spacy==2.1` + `neuralcoref` + `allennlp`\n",
    "* (Working in SageMaker) set instance type to [`ml.g4dn.xlarge`](https://instances.vantage.sh/aws/ec2/g4dn.xlarge) to get NVIDIA T4 GPU\n",
    "* Having issues with `gcc` and other CLang stuff, also `jsonnet` is proving tricky, reverting to EC2 instance...\n",
    "    * pushing only .ipynb changes to central repo\n",
    "\n",
    "**Neuralcoref**\n",
    "* Looks like the neuralcoref team *do not* wish to support GPU inference, nonetheless, it appears possible from the following PR [huggingface/neuralcoref/pull/149](https://github.com/huggingface/neuralcoref/pull/149)\n",
    "    * This is going to be fun and games w/ dependencies, will spin up an EC2 instance later... attempting AllenNLP CUDA-isation for now.\n",
    "\n",
    "**AllenNLP**\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb256d5-588f-416f-abc6-6488d93a329e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## installation and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3062a0ea-584d-41ed-8965-d076ab628edc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==2.1\n",
      "  Using cached spacy-2.1.0.tar.gz (27.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[335 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting setuptools\n",
      "  \u001b[31m   \u001b[0m   Downloading setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n",
      "  \u001b[31m   \u001b[0m      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 11.4 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m Collecting wheel>0.32.0.<0.33.0\n",
      "  \u001b[31m   \u001b[0m   Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting Cython\n",
      "  \u001b[31m   \u001b[0m   Using cached Cython-0.29.32-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "  \u001b[31m   \u001b[0m Collecting cymem<2.1.0,>=2.0.2\n",
      "  \u001b[31m   \u001b[0m   Using cached cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting preshed<2.1.0,>=2.0.1\n",
      "  \u001b[31m   \u001b[0m   Using cached preshed-2.0.1.tar.gz (113 kB)\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  \u001b[31m   \u001b[0m   Using cached murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting thinc==7.0.0.dev6\n",
      "  \u001b[31m   \u001b[0m   Using cached thinc-7.0.0.dev6.tar.gz (1.9 MB)\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting blis<0.3.0,>=0.2.1\n",
      "  \u001b[31m   \u001b[0m   Using cached blis-0.2.4.tar.gz (1.5 MB)\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting thinc_gpu_ops<0.1.0,>=0.0.1\n",
      "  \u001b[31m   \u001b[0m   Using cached thinc_gpu_ops-0.0.4.tar.gz (483 kB)\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting numpy>=1.7.0\n",
      "  \u001b[31m   \u001b[0m   Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "  \u001b[31m   \u001b[0m      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 37.7 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m Collecting wrapt<1.11.0,>=1.10.0\n",
      "  \u001b[31m   \u001b[0m   Using cached wrapt-1.10.11.tar.gz (27 kB)\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting plac<1.0.0,>=0.9.6\n",
      "  \u001b[31m   \u001b[0m   Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting tqdm<5.0.0,>=4.10.0\n",
      "  \u001b[31m   \u001b[0m   Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting six<2.0.0,>=1.10.0\n",
      "  \u001b[31m   \u001b[0m   Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting wasabi<1.1.0,>=0.0.9\n",
      "  \u001b[31m   \u001b[0m   Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting srsly<1.1.0,>=0.0.5\n",
      "  \u001b[31m   \u001b[0m   Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "  \u001b[31m   \u001b[0m      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.1/211.1 kB 3.8 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m Building wheels for collected packages: thinc, preshed, blis, thinc_gpu_ops, wrapt\n",
      "  \u001b[31m   \u001b[0m   Building wheel for thinc (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for thinc (setup.py): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   error: subprocess-exited-with-error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   × python setup.py bdist_wheel did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   │ exit code: 1\n",
      "  \u001b[31m   \u001b[0m   ╰─> [161 lines of output]\n",
      "  \u001b[31m   \u001b[0m       running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m       running build\n",
      "  \u001b[31m   \u001b[0m       running build_py\n",
      "  \u001b[31m   \u001b[0m       creating build\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/t2t.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/v2v.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/about.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/describe.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/misc.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/check.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/t2v.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/exceptions.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/loss.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/i2v.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/__init__.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/api.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/rates.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/compat.py -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/conftest.py -> build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/util.py -> build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/test_util.py -> build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/strategies.py -> build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/test_api_funcs.py -> build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/__init__.py -> build/lib.linux-x86_64-3.8/thinc/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_beam_search.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_loss.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_mem.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_check_exceptions.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_feature_extracter.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_pooling.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_ops.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_pytorch_wrapper.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_hash_embed.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_rnn.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_about.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_rates.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_model.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_difference.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_imports.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/test_affine.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/unit/__init__.py -> build/lib.linux-x86_64-3.8/thinc/tests/unit\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_feed_forward.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_roundtrip_bytes.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_batch_norm.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_basic_tagger.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_mnist.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_affine_learns.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_pickle.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/__init__.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/integration/test_shape_check.py -> build/lib.linux-x86_64-3.8/thinc/tests/integration\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/tests/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/linear/test_linear.py -> build/lib.linux-x86_64-3.8/thinc/tests/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/linear/__init__.py -> build/lib.linux-x86_64-3.8/thinc/tests/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/linear/test_sparse_array.py -> build/lib.linux-x86_64-3.8/thinc/tests/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/tests/linear/test_avgtron.py -> build/lib.linux-x86_64-3.8/thinc/tests/linear\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/__init__.py -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/pooling.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/util.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/vec2vec.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/vecs2vec.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/mem.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/__init__.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/train.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_lsuv.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/vecs2vecs.py -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/hpbff.py -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/wrappers.py -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/datasets.py -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/load_nlp.py -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/__init__.py -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/maxout.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/layernorm.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/convolution.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/attention.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/hash_embed.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/relu.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/elu.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/softmax.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/resnet.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/feed_forward.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/function_layer.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/selu.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/embed.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/model.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/static_vectors.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/batchnorm.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/rnn.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/__init__.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/difference.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/affine.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_classes/feature_extracter.py -> build/lib.linux-x86_64-3.8/thinc/neural/_classes\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/thinc/extra/_vendorized\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/_vendorized/keras_datasets.py -> build/lib.linux-x86_64-3.8/thinc/extra/_vendorized\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/_vendorized/keras_data_utils.py -> build/lib.linux-x86_64-3.8/thinc/extra/_vendorized\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/_vendorized/__init__.py -> build/lib.linux-x86_64-3.8/thinc/extra/_vendorized\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/_vendorized/keras_generic_utils.py -> build/lib.linux-x86_64-3.8/thinc/extra/_vendorized\n",
      "  \u001b[31m   \u001b[0m       copying thinc/typedefs.pyx -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/structs.pyx -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linalg.pyx -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/cpu.pxd -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/structs.pxd -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/typedefs.pxd -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linalg.pxd -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/__init__.pxd -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/compile_time_constants.pxi -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/typedefs.cpp -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linalg.cpp -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/structs.cpp -> build/lib.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/features.pyx -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/linear.pyx -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/sparse.pyx -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/avgtron.pyx -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/serialize.pyx -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/features.pxd -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/avgtron.pxd -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/sparse.pxd -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/__init__.pxd -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/serialize.pxd -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/serialize.cpp -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/avgtron.cpp -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/sparse.cpp -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/features.cpp -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/linear/linear.cpp -> build/lib.linux-x86_64-3.8/thinc/linear\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_aligned_alloc.pyx -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/ops.pyx -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/optimizers.pyx -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/ops.pxd -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/cpu.pxd -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/__init__.pxd -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/ops.cpp -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/optimizers.cpp -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/neural/_aligned_alloc.cpp -> build/lib.linux-x86_64-3.8/thinc/neural\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/search.pyx -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/cache.pyx -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/mb.pyx -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/eg.pyx -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/cache.pxd -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/eg.pxd -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/mb.pxd -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/search.pxd -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/__init__.pxd -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/cache.cpp -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/search.cpp -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/eg.cpp -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       copying thinc/extra/mb.cpp -> build/lib.linux-x86_64-3.8/thinc/extra\n",
      "  \u001b[31m   \u001b[0m       running build_ext\n",
      "  \u001b[31m   \u001b[0m       building 'thinc.linalg' extension\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.8/thinc\n",
      "  \u001b[31m   \u001b[0m       gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.8 -I/tmp/pip-install-f_z085vh/thinc_b352059c4ae14579b598af3c216b6b46/include -I/opt/conda/include/python3.8 -c thinc/linalg.cpp -o build/temp.linux-x86_64-3.8/thinc/linalg.o -O3 -Wno-strict-prototypes -Wno-unused-function\n",
      "  \u001b[31m   \u001b[0m       unable to execute 'gcc': No such file or directory\n",
      "  \u001b[31m   \u001b[0m       error: command 'gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m       [end of output]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m   ERROR: Failed building wheel for thinc\n",
      "  \u001b[31m   \u001b[0m   Running setup.py clean for thinc\n",
      "  \u001b[31m   \u001b[0m   Building wheel for preshed (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for preshed (setup.py): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   error: subprocess-exited-with-error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   × python setup.py bdist_wheel did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   │ exit code: 1\n",
      "  \u001b[31m   \u001b[0m   ╰─> [25 lines of output]\n",
      "  \u001b[31m   \u001b[0m       running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m       running build\n",
      "  \u001b[31m   \u001b[0m       running build_py\n",
      "  \u001b[31m   \u001b[0m       creating build\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       copying preshed/__init__.py -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       copying preshed/about.py -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/preshed/tests\n",
      "  \u001b[31m   \u001b[0m       copying preshed/tests/test_pop.py -> build/lib.linux-x86_64-3.8/preshed/tests\n",
      "  \u001b[31m   \u001b[0m       copying preshed/tests/__init__.py -> build/lib.linux-x86_64-3.8/preshed/tests\n",
      "  \u001b[31m   \u001b[0m       copying preshed/tests/test_hashing.py -> build/lib.linux-x86_64-3.8/preshed/tests\n",
      "  \u001b[31m   \u001b[0m       copying preshed/tests/test_counter.py -> build/lib.linux-x86_64-3.8/preshed/tests\n",
      "  \u001b[31m   \u001b[0m       copying preshed/maps.pyx -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       copying preshed/counter.pyx -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       copying preshed/__init__.pxd -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       copying preshed/maps.pxd -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       copying preshed/counter.pxd -> build/lib.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       running build_ext\n",
      "  \u001b[31m   \u001b[0m       building 'preshed.maps' extension\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.8/preshed\n",
      "  \u001b[31m   \u001b[0m       gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.8 -I/opt/conda/include/python3.8 -c preshed/maps.cpp -o build/temp.linux-x86_64-3.8/preshed/maps.o -O3 -Wno-strict-prototypes -Wno-unused-function\n",
      "  \u001b[31m   \u001b[0m       unable to execute 'gcc': No such file or directory\n",
      "  \u001b[31m   \u001b[0m       error: command 'gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m       [end of output]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m   ERROR: Failed building wheel for preshed\n",
      "  \u001b[31m   \u001b[0m   Running setup.py clean for preshed\n",
      "  \u001b[31m   \u001b[0m   Building wheel for blis (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for blis (setup.py): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   error: subprocess-exited-with-error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   × python setup.py bdist_wheel did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   │ exit code: 1\n",
      "  \u001b[31m   \u001b[0m   ╰─> [31 lines of output]\n",
      "  \u001b[31m   \u001b[0m       BLIS_COMPILER? None\n",
      "  \u001b[31m   \u001b[0m       running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m       running build\n",
      "  \u001b[31m   \u001b[0m       running build_py\n",
      "  \u001b[31m   \u001b[0m       creating build\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       copying blis/__init__.py -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       copying blis/about.py -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       copying blis/benchmark.py -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.8/blis/tests\n",
      "  \u001b[31m   \u001b[0m       copying blis/tests/__init__.py -> build/lib.linux-x86_64-3.8/blis/tests\n",
      "  \u001b[31m   \u001b[0m       copying blis/tests/common.py -> build/lib.linux-x86_64-3.8/blis/tests\n",
      "  \u001b[31m   \u001b[0m       copying blis/tests/test_dotv.py -> build/lib.linux-x86_64-3.8/blis/tests\n",
      "  \u001b[31m   \u001b[0m       copying blis/tests/test_gemm.py -> build/lib.linux-x86_64-3.8/blis/tests\n",
      "  \u001b[31m   \u001b[0m       copying blis/cy.pyx -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       copying blis/py.pyx -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       copying blis/__init__.pxd -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       copying blis/cy.pxd -> build/lib.linux-x86_64-3.8/blis\n",
      "  \u001b[31m   \u001b[0m       running build_ext\n",
      "  \u001b[31m   \u001b[0m       /opt/conda/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/pip-install-f_z085vh/blis_39acff6b0c9d4b3aaade83da561f9c84/blis/cy.pxd\n",
      "  \u001b[31m   \u001b[0m         tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  \u001b[31m   \u001b[0m       /opt/conda/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/pip-install-f_z085vh/blis_39acff6b0c9d4b3aaade83da561f9c84/blis/py.pyx\n",
      "  \u001b[31m   \u001b[0m         tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  \u001b[31m   \u001b[0m       Processing blis/cy.pyx\n",
      "  \u001b[31m   \u001b[0m       Processing blis/py.pyx\n",
      "  \u001b[31m   \u001b[0m       unix\n",
      "  \u001b[31m   \u001b[0m       py_compiler gcc\n",
      "  \u001b[31m   \u001b[0m       {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/wheelwright/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/wheelwright/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'OMP_NUM_THREADS': '1', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/wheelwright/env3.6/bin/python'}\n",
      "  \u001b[31m   \u001b[0m       gcc -c /tmp/pip-install-f_z085vh/blis_39acff6b0c9d4b3aaade83da561f9c84/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmpvtqvl0oq/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-f_z085vh/blis_39acff6b0c9d4b3aaade83da561f9c84/blis/_src/include/linux-x86_64\n",
      "  \u001b[31m   \u001b[0m       error: [Errno 2] No such file or directory: 'gcc'\n",
      "  \u001b[31m   \u001b[0m       [end of output]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m   ERROR: Failed building wheel for blis\n",
      "  \u001b[31m   \u001b[0m   Running setup.py clean for blis\n",
      "  \u001b[31m   \u001b[0m   Building wheel for thinc_gpu_ops (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for thinc_gpu_ops (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Created wheel for thinc_gpu_ops: filename=thinc_gpu_ops-0.0.4-py3-none-any.whl size=54967 sha256=b47a9523a07c2404def34383054fb05ede74cd41b6870531991bf1e439a9799e\n",
      "  \u001b[31m   \u001b[0m   Stored in directory: /root/.cache/pip/wheels/2f/9d/4b/dcdd6bad8c5aa1362b504f727f1a98ac4f47e20d7e4002267d\n",
      "  \u001b[31m   \u001b[0m   Building wheel for wrapt (setup.py): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Created wheel for wrapt: filename=wrapt-1.10.11-py3-none-any.whl size=20757 sha256=0a21cd884a1105a8f5dfd4f25349da726a3a5d87c16afa8d89dbf1b64e67650b\n",
      "  \u001b[31m   \u001b[0m   Stored in directory: /root/.cache/pip/wheels/29/85/3c/c3bd5e63841613b25dac3f6b30f4e2ddb042e1578aa24e3792\n",
      "  \u001b[31m   \u001b[0m Successfully built thinc_gpu_ops wrapt\n",
      "  \u001b[31m   \u001b[0m Failed to build thinc preshed blis\n",
      "  \u001b[31m   \u001b[0m Installing collected packages: wrapt, wasabi, plac, cymem, wheel, tqdm, srsly, six, setuptools, preshed, numpy, murmurhash, Cython, thinc_gpu_ops, blis, thinc\n",
      "  \u001b[31m   \u001b[0m   Running setup.py install for preshed: started\n",
      "  \u001b[31m   \u001b[0m   Running setup.py install for preshed: finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   error: subprocess-exited-with-error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   × Running setup.py install for preshed did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   │ exit code: 1\n",
      "  \u001b[31m   \u001b[0m   ╰─> [8 lines of output]\n",
      "  \u001b[31m   \u001b[0m       running install\n",
      "  \u001b[31m   \u001b[0m       running build\n",
      "  \u001b[31m   \u001b[0m       running build_py\n",
      "  \u001b[31m   \u001b[0m       running build_ext\n",
      "  \u001b[31m   \u001b[0m       building 'preshed.maps' extension\n",
      "  \u001b[31m   \u001b[0m       gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.8 -I/opt/conda/include/python3.8 -c preshed/maps.cpp -o build/temp.linux-x86_64-3.8/preshed/maps.o -O3 -Wno-strict-prototypes -Wno-unused-function\n",
      "  \u001b[31m   \u001b[0m       unable to execute 'gcc': No such file or directory\n",
      "  \u001b[31m   \u001b[0m       error: command 'gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m       [end of output]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m error: legacy-install-failure\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m × Encountered error while trying to install package.\n",
      "  \u001b[31m   \u001b[0m ╰─> preshed\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m note: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m hint: See above for output from the failure.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h/opt/conda/bin/python: No module named spacy\n",
      "\u001b[33mDEPRECATION: --no-binary currently disables reading from the cache of locally built wheels. In the future --no-binary will not influence the wheel cache. pip 23.1 will enforce this behaviour change. A possible replacement is to use the --no-cache-dir option. You can use the flag --use-feature=no-binary-enable-wheel-cache to test the upcoming behaviour. Discussion can be found at https://github.com/pypa/pip/issues/11453\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting neuralcoref\n",
      "  Using cached neuralcoref-4.0.tar.gz (368 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from neuralcoref) (1.23.5)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from neuralcoref) (1.26.24)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from neuralcoref) (2.28.1)\n",
      "Collecting spacy>=2.1.0\n",
      "  Using cached spacy-3.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.13)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Using cached thinc-8.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (819 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (4.62.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Using cached pydantic-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (3.1.2)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (58.0.4)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->neuralcoref) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.24 in /opt/conda/lib/python3.8/site-packages (from boto3->neuralcoref) (1.29.24)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3->neuralcoref) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.24->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy>=2.1.0->neuralcoref) (3.0.4)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting typing-extensions>=4.1.0\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy>=2.1.0->neuralcoref) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy>=2.1.0->neuralcoref) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.24->boto3->neuralcoref) (1.16.0)\n",
      "Installing collected packages: wasabi, cymem, typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, srsly, pydantic, preshed, pathy, confection, thinc, spacy, neuralcoref\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "\u001b[33m  DEPRECATION: neuralcoref is being installed using the legacy 'setup.py install' method, because the '--no-binary' option was enabled for it and this currently disables local wheel building for projects that don't have a 'pyproject.toml' file. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/11451\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for neuralcoref ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for neuralcoref\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/neuralcoref\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/__init__.py -> build/lib.linux-x86_64-3.8/neuralcoref\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/file_utils.py -> build/lib.linux-x86_64-3.8/neuralcoref\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/neuralcoref/tests\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/tests/__init__.py -> build/lib.linux-x86_64-3.8/neuralcoref/tests\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/tests/test_neuralcoref.py -> build/lib.linux-x86_64-3.8/neuralcoref/tests\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/__init__.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/algorithm.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/compat.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/conllparser.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/dataset.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/document.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/evaluator.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/learn.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/model.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m copying neuralcoref/train/utils.py -> build/lib.linux-x86_64-3.8/neuralcoref/train\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'neuralcoref.neuralcoref' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8/neuralcoref\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.8 -I/tmp/pip-install-ytgwgi0i/neuralcoref_373ed4627cf24380bc6a3207597269cb/include -I/opt/conda/include/python3.8 -c neuralcoref/neuralcoref.cpp -o build/temp.linux-x86_64-3.8/neuralcoref/neuralcoref.o -O2 -Wno-strict-prototypes -Wno-unused-function\n",
      "  \u001b[31m   \u001b[0m unable to execute 'gcc': No such file or directory\n",
      "  \u001b[31m   \u001b[0m error: command 'gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m neuralcoref\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\u001b[?25hCollecting allennlp\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /opt/conda/lib/python3.8/site-packages (from allennlp) (1.23.5)\n",
      "Requirement already satisfied: traitlets>5.1.1 in /opt/conda/lib/python3.8/site-packages (from allennlp) (5.6.0)\n",
      "Requirement already satisfied: nltk>=3.6.5 in /opt/conda/lib/python3.8/site-packages (from allennlp) (3.6.5)\n",
      "Collecting spacy<3.4,>=2.1.0\n",
      "  Downloading spacy-3.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting scipy>=1.7.3\n",
      "  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
      "  Using cached wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting scikit-learn>=1.0.1\n",
      "  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision<0.14.0,>=0.8.1\n",
      "  Using cached torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.8/site-packages (from allennlp) (0.3.6)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in /opt/conda/lib/python3.8/site-packages (from allennlp) (3.3.1)\n",
      "Collecting h5py>=3.6.0\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboardX>=1.2\n",
      "  Using cached tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from allennlp) (3.20.3)\n",
      "Collecting termcolor==1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
      "  Using cached cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.8/site-packages (from allennlp) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62 in /opt/conda/lib/python3.8/site-packages (from allennlp) (4.62.3)\n",
      "Collecting more-itertools>=8.12.0\n",
      "  Using cached more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.6/593.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from allennlp) (0.7.0)\n",
      "Collecting sentencepiece>=0.1.96\n",
      "  Using cached sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting torch<1.13.0,>=1.10.0\n",
      "  Using cached torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "Collecting lmdb>=1.2.1\n",
      "  Downloading lmdb-1.4.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.1/306.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fairscale==0.4.6\n",
      "  Using cached fairscale-0.4.6.tar.gz (248 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<4.21,>=4.1\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Collecting base58>=2.1.1\n",
      "  Using cached base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting pytest>=6.2.5\n",
      "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.2/110.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich<13.0,>=12.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock<3.8,>=3.3\n",
      "  Using cached filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Using cached huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.26.24)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.0.16->allennlp) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.0.16->allennlp) (5.4.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->allennlp) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->allennlp) (2021.8.3)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp) (0.13.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp) (21.2.0)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp) (1.26.13)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0.1->allennlp) (2.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Using cached thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Collecting typer>=0.4.1\n",
      "  Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting protobuf<4.0.0,>=3.12.0\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.3.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Using cached GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting pathtools\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.8.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.24 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.29.24)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Using cached google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Using cached google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.0/177.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.10.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.24->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.0/218.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<4.0.0,>=3.12.0\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.7.2)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
      "Building wheels for collected packages: fairscale, termcolor, jsonnet, sacremoses, promise, pathtools\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307224 sha256=8db284c1e281f32acefdcd92b8a325959d3edbd9d742402ffd497f521bd5e251\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/e8/f1/4f2cc869823c35e834c6cee0552a0605c2bdc89f7da81f1a1d\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=cd93c1e47377bcc50131ee043fe6136e776e43ba00952182927706352096c654\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/de/e8/f6aec43099b5f725a5227572d0d67a987bc444298b4f14d745\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[4 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: 'make'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for jsonnet\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for jsonnet\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=372093d006cf02d313fb1f880eecd6f2eb3a017e0a00a2eb159b65367bf97ce0\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/a3/ff/01dc060d7fc51176b3ce7cf1561466a12e658164b594747547\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=0ba02bf73c58b2b741adf970e615fde175f57360792cc59de5f7db4593971feb\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/fe/dc/a7b3e03dfd0afb3a19691905bbafac1fbaebb704a02a4daeb2\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=fa0b0f684b85fbe07fe334e642ac109138b0ad045fe4d3942f4081ba97be8a25\n",
      "  Stored in directory: /root/.cache/pip/wheels/4d/33/74/7c0903053e955973d5dc3d21857a29b3f8c0806ad0b05c32a1\n",
      "Successfully built fairscale termcolor sacremoses promise pathtools\n",
      "Failed to build jsonnet\n",
      "Installing collected packages: tokenizers, termcolor, sentencepiece, pathtools, lmdb, jsonnet, commonmark, typer, torch, smmap, shortuuid, setproctitle, sentry-sdk, scipy, sacremoses, rich, pydantic, pyasn1-modules, protobuf, promise, more-itertools, h5py, google-crc32c, filelock, exceptiongroup, docker-pycreds, cachetools, base58, torchvision, thinc, tensorboardX, scikit-learn, pytest, huggingface-hub, googleapis-common-protos, google-resumable-media, google-auth, gitdb, fairscale, transformers, spacy, google-api-core, GitPython, wandb, google-cloud-core, google-cloud-storage, cached-path, allennlp\n",
      "  Running setup.py install for jsonnet ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for jsonnet\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[4 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: 'make'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m jsonnet\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\u001b[?25hCollecting allennlp-models\n",
      "  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /opt/conda/lib/python3.8/site-packages (from allennlp-models) (3.6.5)\n",
      "Collecting torch<1.13.0,>=1.7.0\n",
      "  Using cached torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "Collecting py-rouge==1.1\n",
      "  Using cached py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "Collecting conllu==4.4.2\n",
      "  Using cached conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting word2number>=1.1\n",
      "  Using cached word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting allennlp<2.11,>=2.10.1\n",
      "  Using cached allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.3.6)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.1.97)\n",
      "Collecting fairscale==0.4.6\n",
      "  Using cached fairscale-0.4.6-py3-none-any.whl\n",
      "Collecting cached-path<1.2.0,>=1.1.3\n",
      "  Using cached cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.21.4 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.23.5)\n",
      "Collecting wandb<0.13.0,>=0.10.0\n",
      "  Using cached wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.62 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (4.62.3)\n",
      "Collecting scipy>=1.7.3\n",
      "  Downloading scipy-1.10.0rc1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting more-itertools>=8.12.0\n",
      "  Using cached more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Using cached tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.4.0)\n",
      "Collecting base58>=2.1.1\n",
      "  Using cached base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting torchvision<0.14.0,>=0.8.1\n",
      "  Using cached torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "Requirement already satisfied: traitlets>5.1.1 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (5.6.0)\n",
      "Collecting pytest>=6.2.5\n",
      "  Using cached pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "Collecting scikit-learn>=1.0.1\n",
      "  Using cached scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.3.1)\n",
      "Collecting jsonnet>=0.10.0\n",
      "  Using cached jsonnet-0.19.1.tar.gz (593 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting spacy<3.4,>=2.1.0\n",
      "  Using cached spacy-3.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.20.3)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.1.0)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: typer>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.7.0)\n",
      "Collecting h5py>=3.6.0\n",
      "  Using cached h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "Collecting transformers<4.21,>=4.1\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.8/site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (2.28.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->allennlp-models) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->allennlp-models) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->allennlp-models) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch<1.13.0,>=1.7.0->allennlp-models) (4.4.0)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-4.0.0a1.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets->allennlp-models) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets->allennlp-models) (10.0.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets->allennlp-models) (0.70.14)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets->allennlp-models) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets->allennlp-models) (5.4.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.8/site-packages (from ftfy->allennlp-models) (0.2.5)\n",
      "Collecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Using cached google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.26.24)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Using cached huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Collecting filelock<3.8,>=3.3\n",
      "  Using cached filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting rich<13.0,>=12.1\n",
      "  Using cached rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->allennlp-models) (2.0.4)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->allennlp-models) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets->allennlp-models) (3.0.4)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Using cached exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (0.13.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.1->allennlp-models) (2.2.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (58.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.7.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.10)\n",
      "Collecting typer>=0.4.1\n",
      "  Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.8/site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.4.5)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Using cached thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "Collecting protobuf<4.0.0,>=3.12.0\n",
      "  Using cached protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision<0.14.0,>=0.8.1->allennlp<2.11,>=2.10.1->allennlp-models) (9.3.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers<4.21,>=4.1->allennlp<2.11,>=2.10.1->allennlp-models) (0.12.1)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp-models) (5.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.16.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.1.2)\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Using cached GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Using cached sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->allennlp-models) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->allennlp-models) (2021.3)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.24 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.29.24)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Using cached google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Using cached google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Using cached google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (6.3.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "Collecting protobuf<4.0.0,>=3.12.0\n",
      "  Downloading protobuf-3.20.1rc1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0rc1-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (4.7.2)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.4.8)\n",
      "Building wheels for collected packages: word2number, jsonnet\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=1198d0713697227f3a9cbbf1ead762e565b5abe071b9799f39279dc7e6c42573\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/b0/06/db68600ecd8bd5f1ab23b17469940cb7db4c547895ee9c6875\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[4 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: 'make'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for jsonnet\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for jsonnet\n",
      "Successfully built word2number\n",
      "Failed to build jsonnet\n",
      "Installing collected packages: word2number, py-rouge, jsonnet, commonmark, xxhash, typer, torch, smmap, shortuuid, setproctitle, sentry-sdk, scipy, sacremoses, rich, pydantic, pyasn1-modules, protobuf, promise, multidict, more-itertools, h5py, google-crc32c, ftfy, fsspec, frozenlist, filelock, exceptiongroup, docker-pycreds, conllu, cachetools, base58, async-timeout, yarl, torchvision, thinc, tensorboardX, scikit-learn, responses, pytest, huggingface-hub, googleapis-common-protos, google-resumable-media, google-auth, gitdb, fairscale, aiosignal, transformers, spacy, google-api-core, GitPython, aiohttp, wandb, google-cloud-core, google-cloud-storage, datasets, cached-path, allennlp, allennlp-models\n",
      "  Running setup.py install for jsonnet ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for jsonnet\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[4 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: 'make'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m jsonnet\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.6.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.6.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.17.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.7.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.7.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.8)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.4.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.17.3)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (2.16.2)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.5.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (5.10.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.4)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.8)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.2)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (7.2.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.10.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.0.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.59.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.6.2)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.2)\n",
      "Requirement already satisfied: jupyter-events>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.8/site-packages (from nbclient>=0.5.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.4.1)\n",
      "Requirement already satisfied: python-json-logger in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (20.11.0)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: rfc3986-validator>0.1.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.1.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (2.3)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (1.12)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.8/site-packages (from isoduration->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (1.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# change me from raw > code to execute\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "!pip install spacy==2.1\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install neuralcoref --no-binary neuralcoref\n",
    "!pip install allennlp\n",
    "!pip install --pre allennlp-models\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f6f043-30e6-4e51-a45b-01daa02864a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneuralcoref\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Predictor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "from wasabi import msg\n",
    "from allennlp.predictors.predictor import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7a758-3f6d-42ec-a447-5c8f7e45c25d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## creating neuralcoref function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985eafa4-8caa-4c52-9dda-9475c746ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(text):\n",
    "    # TODO: move outside function & make class-based\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    neuralcoref.add_to_pipe(nlp)\n",
    "    doc = nlp(text)\n",
    "    out = {\n",
    "        \"resolved\": doc._.coref_resolved,\n",
    "        \"clusters\": doc._.coref_clusters,\n",
    "        \"token_data\": [[token.text, token.pos_, token.tag_]  for token in doc]\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52fd41f4-bf8f-40fd-944d-64d19ef0cb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Luca: [Luca, his, him]]\n"
     ]
    }
   ],
   "source": [
    "text = 'Luca sat at his desk, before Lily interrupted him.'\n",
    "data = neural(text)\n",
    "print(data['clusters'])\n",
    "# not ideal performance..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751dcbb-81ee-4351-b714-2d1c903b4a09",
   "metadata": {},
   "source": [
    "## creating AllenNLP function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53376716-4037-488b-944d-81a35080a7fa",
   "metadata": {},
   "source": [
    "**N.B.** the `coref-spanbert-large` model must be downloaded to the notebook directory as below.\n",
    "```console\n",
    "sagemaker-user@studio$ conda activate base\n",
    "(base) sagemaker-user@studio$ wget https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27509082-0d1c-45d9-bb01-d16ede9d997e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_url = 'coref-spanbert-large-2020.02.27.tar.gz'\n",
    "predictor = Predictor.from_path(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bc7f3d0-e5aa-4057-9d18-fd1edbd789d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0], [3, 3]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Luca sat at his desk, before Lily interrupted him.'\n",
    "prediction = predictor.predict(document=text)\n",
    "' '.join(prediction['document'])\n",
    "prediction['clusters']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a408206-b2ea-429c-a4ae-b5b64a1aab97",
   "metadata": {},
   "source": [
    "### intersection strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72a46524-dadb-42de-9666-e01acbd89eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_models, print_clusters\n",
    "from utils import IntersectionStrategy, StrictIntersectionStrategy, PartialIntersectionStrategy, FuzzyIntersectionStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06ce4259-977a-42ee-afd9-4d729625e0a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.common.plugins:Plugin allennlp_models available\n",
      "INFO:cached_path:cache of https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz is up-to-date\n",
      "INFO:allennlp.models.archival:loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /root/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "INFO:allennlp.models.archival:extracting archive file /root/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /tmp/tmpgbvtxgl1\n",
      "INFO:allennlp.common.params:dataset_reader.type = coref\n",
      "INFO:allennlp.common.params:dataset_reader.max_instances = None\n",
      "INFO:allennlp.common.params:dataset_reader.manual_distributed_sharding = False\n",
      "INFO:allennlp.common.params:dataset_reader.manual_multiprocess_sharding = False\n",
      "INFO:allennlp.common.params:dataset_reader.max_span_width = 30\n",
      "INFO:allennlp.common.params:dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "INFO:allennlp.common.params:dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "INFO:allennlp.common.params:dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "INFO:allennlp.common.params:dataset_reader.token_indexers.tokens.namespace = tags\n",
      "INFO:allennlp.common.params:dataset_reader.token_indexers.tokens.max_length = 512\n",
      "INFO:allennlp.common.params:dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "INFO:allennlp.common.params:dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "INFO:allennlp.common.params:dataset_reader.max_sentences = 110\n",
      "INFO:allennlp.common.params:dataset_reader.remove_singleton_clusters = False\n",
      "INFO:allennlp.common.params:validation_dataset_reader.type = coref\n",
      "INFO:allennlp.common.params:validation_dataset_reader.max_instances = None\n",
      "INFO:allennlp.common.params:validation_dataset_reader.manual_distributed_sharding = False\n",
      "INFO:allennlp.common.params:validation_dataset_reader.manual_multiprocess_sharding = False\n",
      "INFO:allennlp.common.params:validation_dataset_reader.max_span_width = 30\n",
      "INFO:allennlp.common.params:validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "INFO:allennlp.common.params:validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "INFO:allennlp.common.params:validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "INFO:allennlp.common.params:validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "INFO:allennlp.common.params:validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "INFO:allennlp.common.params:validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "INFO:allennlp.common.params:validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "INFO:allennlp.common.params:validation_dataset_reader.max_sentences = None\n",
      "INFO:allennlp.common.params:validation_dataset_reader.remove_singleton_clusters = False\n",
      "INFO:allennlp.common.params:type = from_instances\n",
      "INFO:allennlp.data.vocabulary:Loading token dictionary from /tmp/tmpgbvtxgl1/vocabulary.\n",
      "INFO:allennlp.common.params:model.type = coref\n",
      "INFO:allennlp.common.params:model.regularizer = None\n",
      "INFO:allennlp.common.params:model.ddp_accelerator = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.type = basic\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
      "INFO:allennlp.common.params:model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
      "INFO:allennlp.common.params:model.context_layer.type = pass_through\n",
      "INFO:allennlp.common.params:model.context_layer.input_dim = 1024\n",
      "INFO:allennlp.common.params:model.mention_feedforward.input_dim = 3092\n",
      "INFO:allennlp.common.params:model.mention_feedforward.num_layers = 2\n",
      "INFO:allennlp.common.params:model.mention_feedforward.hidden_dims = 1500\n",
      "INFO:allennlp.common.params:model.mention_feedforward.activations = relu\n",
      "INFO:allennlp.common.params:type = relu\n",
      "INFO:allennlp.common.params:model.mention_feedforward.dropout = 0.3\n",
      "INFO:allennlp.common.params:model.antecedent_feedforward.input_dim = 9296\n",
      "INFO:allennlp.common.params:model.antecedent_feedforward.num_layers = 2\n",
      "INFO:allennlp.common.params:model.antecedent_feedforward.hidden_dims = 1500\n",
      "INFO:allennlp.common.params:model.antecedent_feedforward.activations = relu\n",
      "INFO:allennlp.common.params:type = relu\n",
      "INFO:allennlp.common.params:model.antecedent_feedforward.dropout = 0.3\n",
      "INFO:allennlp.common.params:model.feature_size = 20\n",
      "INFO:allennlp.common.params:model.max_span_width = 30\n",
      "INFO:allennlp.common.params:model.spans_per_word = 0.4\n",
      "INFO:allennlp.common.params:model.max_antecedents = 50\n",
      "INFO:allennlp.common.params:model.coarse_to_fine = True\n",
      "INFO:allennlp.common.params:model.inference_order = 2\n",
      "INFO:allennlp.common.params:model.lexical_dropout = 0.2\n",
      "INFO:allennlp.common.params:model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f1e824dfaf0>\n",
      "INFO:allennlp.nn.initializers:Initializing parameters\n",
      "INFO:allennlp.nn.initializers:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "INFO:allennlp.nn.initializers:   _antecedent_feedforward._module._linear_layers.0.bias\n",
      "INFO:allennlp.nn.initializers:   _antecedent_feedforward._module._linear_layers.0.weight\n",
      "INFO:allennlp.nn.initializers:   _antecedent_feedforward._module._linear_layers.1.bias\n",
      "INFO:allennlp.nn.initializers:   _antecedent_feedforward._module._linear_layers.1.weight\n",
      "INFO:allennlp.nn.initializers:   _antecedent_scorer._module.bias\n",
      "INFO:allennlp.nn.initializers:   _antecedent_scorer._module.weight\n",
      "INFO:allennlp.nn.initializers:   _attentive_span_extractor._global_attention._module.bias\n",
      "INFO:allennlp.nn.initializers:   _attentive_span_extractor._global_attention._module.weight\n",
      "INFO:allennlp.nn.initializers:   _coarse2fine_scorer.bias\n",
      "INFO:allennlp.nn.initializers:   _coarse2fine_scorer.weight\n",
      "INFO:allennlp.nn.initializers:   _distance_embedding.weight\n",
      "INFO:allennlp.nn.initializers:   _endpoint_span_extractor._span_width_embedding.weight\n",
      "INFO:allennlp.nn.initializers:   _mention_feedforward._module._linear_layers.0.bias\n",
      "INFO:allennlp.nn.initializers:   _mention_feedforward._module._linear_layers.0.weight\n",
      "INFO:allennlp.nn.initializers:   _mention_feedforward._module._linear_layers.1.bias\n",
      "INFO:allennlp.nn.initializers:   _mention_feedforward._module._linear_layers.1.weight\n",
      "INFO:allennlp.nn.initializers:   _mention_scorer._module.bias\n",
      "INFO:allennlp.nn.initializers:   _mention_scorer._module.weight\n",
      "INFO:allennlp.nn.initializers:   _span_updating_gated_sum._gate.bias\n",
      "INFO:allennlp.nn.initializers:   _span_updating_gated_sum._gate.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "INFO:allennlp.modules.token_embedders.embedding:Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "INFO:allennlp.modules.token_embedders.embedding:Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "INFO:allennlp.models.archival:removing temporary unarchived model dir at /tmp/tmpgbvtxgl1\n"
     ]
    }
   ],
   "source": [
    "predictor, nlp = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "993d6fb7-17bd-4598-a05c-42bd00d6a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Austin Jermaine Wiley (born January 8, 1999) is an American basketball player. He currently plays for the Auburn Tigers in the Southeastern Conference. Wiley attended Spain Park High School in Hoover, Alabama, where he averaged 27.1 points, 12.7 rebounds and 2.9 blocked shots as a junior in 2015-16, before moving to Florida, where he went to Calusa Preparatory School in Miami, Florida, while playing basketball at The Conrad Academy in Orlando.\"\n",
    "\n",
    "clusters = predictor.predict(text)['clusters']\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85b0565b-7414-49b4-82e1-9cc99d0761fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ AllenNLP clusters ~~~\n",
      "Austin Jermaine Wiley - [Austin Jermaine Wiley; He; Wiley; he; he]\n",
      "Florida - [Florida; Florida]\n",
      "\n",
      "~~~ Huggingface clusters ~~~\n",
      "Wiley: [Austin Jermaine Wiley (born January 8, 1999), He, Wiley, he, he]\n",
      "Florida: [Florida, Florida]\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~ AllenNLP clusters ~~~\")\n",
    "print_clusters(doc, clusters)\n",
    "print(\"\\n~~~ Huggingface clusters ~~~\")\n",
    "for cluster in doc._.coref_clusters:\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ebd3b0d-a2d1-4fc2-9b50-1223c818cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict = StrictIntersectionStrategy(predictor, nlp)\n",
    "partial = PartialIntersectionStrategy(predictor, nlp)\n",
    "fuzzy = FuzzyIntersectionStrategy(predictor, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c1925aa-e05e-41ff-b8a6-850eb623c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~ StrictIntersectionStrategy clusters ~~~\n",
      "Florida - [Florida; Florida]\n",
      "\n",
      "~~~ PartialIntersectionStrategy clusters ~~~\n",
      "Wiley - [He; Wiley; he; he]\n",
      "Florida - [Florida; Florida]\n",
      "\n",
      "~~~ FuzzyIntersectionStrategy clusters ~~~\n",
      "Austin Jermaine Wiley - [Austin Jermaine Wiley; He; Wiley; he; he]\n",
      "Florida - [Florida; Florida]\n"
     ]
    }
   ],
   "source": [
    "for intersection_strategy in [strict, partial, fuzzy]:\n",
    "    print(f'\\n~~~ {intersection_strategy.__class__.__name__} clusters ~~~')\n",
    "    print_clusters(doc, intersection_strategy.clusters(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9f700ff-0351-4151-b4fd-715b92fd6f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin Jermaine Wiley - [Austin Jermaine Wiley; He; Wiley; he; he]\n",
      "Florida - [Florida; Florida]\n"
     ]
    }
   ],
   "source": [
    "print_clusters(nlp(text), fuzzy.clusters(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdc6f0b1-2330-4d3a-a349-b504ddffa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy = FuzzyIntersectionStrategy(predictor, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fc7bf97-e195-43ac-8ad6-8e2d2ed6188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Austin Jermaine Wiley (born January 8, 1999) is an American basketball player. Austin Jermaine Wiley currently plays for the Auburn Tigers in the Southeastern Conference. Austin Jermaine Wiley attended Spain Park High School in Hoover, Alabama, where Austin Jermaine Wiley averaged 27.1 points, 12.7 rebounds and 2.9 blocked shots as a junior in 2015-16, before moving to Florida, where Austin Jermaine Wiley went to Calusa Preparatory School in Miami, Florida, while playing basketball at The Conrad Academy in Orlando.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy.resolve_coreferences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ddf1e26-b5c2-4115-a86d-d2f95a333129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 2], [16, 16], [28, 28], [40, 40], [65, 65]], [[62, 62], [74, 74]]]\n"
     ]
    }
   ],
   "source": [
    "print(fuzzy.clusters(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf83653-24e9-4de1-a4b4-7a2533a02f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
